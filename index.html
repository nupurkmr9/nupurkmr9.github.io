<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content='width=800'>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 12px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 12px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22 px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 13px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }



    img {
      display: inline;
      margin: 0 auto;
      width: 100%;
    }

    .image-cropper {
      width: 250px;
      height: 250px;
      position: relative;
      overflow: hidden;
      border-radius: 50%;
    }
  </style>
  <link rel="icon" type="image" href="img/logo.jpg">
  <title>Nupur Kumari</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Nupur Kumari</name>
                </font>
              <p align>I am a final year PhD student at Robotics Institute, Carnegie Mellon University (CMU). I am advised by <a href="https://www.cs.cmu.edu/~junyanz/"> Jun-Yan Zhu</a> and collaborate closely with
                <a href="https://richzhang.github.io/">Richard Zhang</a> and <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>. My research interests lie in computer vision, specifically, generative models, model customization, and post-training techniques.
                <br>
                <br>
                Prior to CMU, I worked at Media and Data Science Research, Adobe India, and had the pleasure to
                collaborate with <a href="https://www.iith.ac.in/~vineethnb/">Vineeth N Balasubramanian</a> during that time. I did my undergraduate from Indian Institute of Tenchnology Delhi with a major in Mathematics and Computing.

              <p align=center>
                <a href="mailto:nupurkmr9@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/nupur-kumari-582369112/">LinkedIn</a> &nbsp/&nbsp
                <a href="files/resume.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=vRWKLJ8AAAAJ&hl=en">Google Scholar</a>

              </p>
            </td>
            <td width="40%"><img class="image-cropper" src="img/photo.jpg"></td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
            <tr>
            <td style="padding:5px;width:100%;vertical-align:middle">
              <heading style="font-size:22px"> News</heading>
            </td>
          </tr>        
            <tr>
            <td style="padding:5px;width:100%;vertical-align:middle">
              <!-- <li>              <font color='red'> <strong>I am actively looking for full-time position and postdoc position.</strong></font> Feel free to reach out to me!</li> -->
              <li>October 2025: Gave a talk at <a href="https://higen-2025.github.io">HiGen</a> and <a href="https://www.cvlai.net/aim/2025/">AIM</a> Workshop at ICCV 2025.</li>
              <li>October 2025: Co-organized the <a href="https://p13n-workshop.github.io/">Personalization in Generative AI</a> Workshop at ICCV2025</li>
              <li>August 2025: Selected as one of the WiGRAPH's <a href="https://www.wigraph.org/spotlights/meet-our-rising-stars-2025/"> 2025 Rising Stars in Computer Graphics</a>!!</li> 
              <li>September 2023: Concept Ablation featured in <a href="https://www.cmu.edu/news/stories/archives/2023/September/addressing-copyright-compensation">CMU News</a></li>
              <li>June 2023: Gave a talk about Custom Diffusion at <a href="https://theaitalks.org/talks/2023/0406/">The AI Talks</a></li>
            </td>            
            </tr>
          </tbody>
        </table>   


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <br>
            <br>
            <br>
            <heading style="font-size:22px"> Selected Publications</heading>
            <br>
            <br>
          </tr>

          <tr>
            <td width="30%">
              <img id="img-opt" src="img/npedit_gif.gif" alt="project_img" width="160" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><a href="https://arxiv.org/abs/2502.01720">
                  <papertitle>NP-Edit: Learning an Image Editing Model without Image Editing Pairs</papertitle>
                </a><br>
                <i><span style="font-size: 10pt;">
                    Nupur Kumari, <a href="https://peterwang512.github.io">Sheng-Yu Wang</a>,
                    <a href="https://www.nxzhao.com">Nanxuan Zhao</a>,
                    <a href="https://yotamnitzan.github.io">Yotam Nitzan</a>,
                    <a href="https://yuheng-li.github.io">Yuheng Li</a>,
                    <a href="https://krsingh.cs.ucdavis.edu">Krishna Kumar Singh</a>,
                    <a href="http://richzhang.github.io">Richard Zhang</a>,
                    <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>,
                    <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>,
                    <a href="https://www.xunhuang.me">Xun Huang</a></i><br>

              <p>We propose NP-Edit (No-Pair Edit), a framework for training image editing models using gradient feedback from a Visionâ€“Language Model (VLM), requiring no paired supervision. Our formulation combines VLM feedback with distribution matching loss to learn a few-step image editing model. We show that performance improves directly with more powerful VLMs and larger datasets, demonstrating its strong potential and scalability.
                <br>

              </p>
              <strong>ArXiv 2025.</strong><br>
              [<a href="https://arxiv.org/abs/2510.14978">Paper</a>]
              [<a href="https://nupurkmr9.github.io/npedit/">Webpage</a>]
              </a> </p>
            </td>
          </tr>

          <tr>
            <td width="30%">
              <img id="img-opt" src="img/syncd.gif" alt="project_img" width="160" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><a href="https://arxiv.org/abs/2502.01720">
                  <papertitle>Generating Multi-Image Synthetic Data for Text-to-Image Customization</papertitle>
                </a><br>
                <i><span style="font-size: 10pt;">
                    Nupur Kumari, <a
                      href="https://xiyinmsu.github.io//">Xi Yin</a>, <a
                      href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>,<a
                      href="https://imisra.github.io/">Ishan Misra</a>, <a
                      href="https://github.com/azadis">Samaneh Azadi</a></i><br>

              <p>We propose a data generation pipeline for image customization consisting of multiple images of the same object in different contexts. Given the training data, we train a new encoder-based model for the task, which can successfully generate new compositions of a reference object using text prompts.
                <br>

              </p>
              <strong>ICCV 2025.</strong><br>
              [<a href="https://arxiv.org/abs/2502.01720">Paper</a>]
              [<a href="https://www.cs.cmu.edu/~syncd-project/">Webpage</a>]
              [<a href="https://github.com/nupurkmr9/syncd">Code</a>]
              </a> </p>
            </td>
          </tr>

          <tr>
            <td width="30%">
              <img id="img-opt" src="img/generative_photomontage.jpg" alt="project_img" width="160" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><a href="https://arxiv.org/abs/2408.07116">
                  <papertitle>Generative Photomontage</papertitle>
                </a><br>
                <i><span style="font-size: 10pt;">
                  <a href="https://lseancs.github.io">Sean J. Liu</a>, Nupur Kumari, 
                  <a href="https://faculty.runi.ac.il/arik/">Ariel Shamir</a>, <a
                  href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a></i><br>

              <p>We propose a framework for creating the desired image by compositing it from various parts of generated images, in essence forming a Generative Photomontage.
                <br>

              </p>
              <strong>CVPR 2025.</strong><br>
              [<a href="https://arxiv.org/abs/2408.07116">Paper</a>]
              [<a href="https://lseancs.github.io/generativephotomontage/">Webpage</a>]
              [<a href="https://github.com/lseancs/GenerativePhotomontage">Code</a>]
              </a> </p>
            </td>
          </tr>

          <tr>
            <td width="30%">
              <img id="img-opt" src="img/customdiffusion360.jpg" alt="project_img" width="160" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><a href="https://arxiv.org/abs/2404.12333">
                  <papertitle>Customizing Text- to-Image Diffusion with Object Viewpoint Control</papertitle>
                </a><br>
                <i><span style="font-size: 10pt;">
                    Nupur Kumari<sup>&#42;</sup>, <a href="https://graceduansu.github.io">Grace Su<sup>&#42;</sup></a>, <a
                      href="https://richzhang.github.io/">Richard Zhang</a>, <a
                      href="https://taesung.me">Taesung Park</a>, <a
                      href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a
                      href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a></i><br>

              <p>We propose Custom Diffusion-360, to add object viewpoint control when personalizing text-to-image diffusion models, e.g. <a
                  href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0">Stable Diffusion-XL</a>, given multi-view images of the new object.
                <br>

              </p>
              <strong>SIGGRAPH Asia 2024.</strong><br>
              [<a href="https://arxiv.org/abs/2404.12333">Paper</a>]
              [<a href="https://customdiffusion360.github.io">Webpage</a>]
              [<a href="https://github.com/customdiffusion360/custom-diffusion360">Code</a>]
              </a> </p>
            </td>
          </tr>


          <tr>
            <td width="30%">
              <img id="img-opt" src="img/pair_customization_mini_teaser.jpg" alt="project_img" width="160" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><a href="https://arxiv.org/abs/2405.01536">
                  <papertitle>Customizing Text-to-Image Models with a Single Image Pair</papertitle>
                </a><br>
                <i><span style="font-size: 10pt;">
                  <a href="https://maxwelljon.es">Maxwell Jones</a>, <a
                  href="https://peterwang512.github.io">Sheng-Yu Wang</a>, Nupur Kumari, <a
                  href="https://baulab.info/"> David Bau</a>, <a
                  href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a></i><br>

              <p>We propose PairCustomization, a method to learn new style concepts from a single image pair by decomposing style and content.
                <br>

              </p>
              <strong>SIGGRAPH Asia 2024.</strong><br>
              [<a href="https://arxiv.org/abs/2404.12333">Paper</a>]
              [<a href="https://paircustomization.github.io/">Webpage</a>]
              [<a href="https://github.com/PairCustomization/PairCustomization">Code</a>]
              </a> </p>
            </td>
          </tr>




          <tr>
            <td width="30%">
              <img id="img-opt" src="img/custom_diffusion.gif" alt="project_img" width="160" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><a href="http://arxiv.org/abs/2212.04488">
                  <papertitle>Multi-Concept Customization of Text-to-Image Diffusion</papertitle>
                </a><br>
                <i><span style="font-size: 10pt;">
                    Nupur Kumari, <a href="https://zhangbingliang2019.github.io">Bingliang Zhang</a>, <a
                      href="https://richzhang.github.io/">Richard Zhang</a>, <a
                      href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a
                      href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a></i><br>

              <p>We propose Custom Diffusion, a method to fine-tune large-scale text-to-image diffusion models, e.g. <a
                  href="https://huggingface.co/CompVis/stable-diffusion-v-1-4-original">Stable Diffusion</a>, given few
                (~4-20) user-provided images of a new concept.
                Our method is computationally efficient (~6 minutes on 2 A100 GPUs) and has low storage requirements for
                each additional concept model (75MB) apart from the pretrained model.
                <br>

              </p>
              <strong>CVPR 2023.</strong><br>
              [<a href="http://arxiv.org/abs/2212.04488">Paper</a>]
              [<a href="https://www.cs.cmu.edu/~custom-diffusion/">Webpage</a>]
              [<a href="https://github.com/adobe-research/custom-diffusion">Code</a>]
              </a> </p>
            </td>
          </tr>


          <tr>
            <td width="30%">
              <img id="img-opt" src="img/concept_ablation.jpg" alt="project_img" width="160" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><a href="https://arxiv.org/abs/2303.13516">
                  <papertitle>Ablating Concepts in Text-to-Image Diffusion Models</papertitle>
                </a><br>
                <i><span style="font-size: 10pt;">
                    Nupur Kumari, <a href="https://zhangbingliang2019.github.io">Bingliang Zhang</a>, <a
                      href="https://peterwang512.github.io">Sheng-Yu Wang</a>, <a
                      href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a
                      href="https://richzhang.github.io/">Richard Zhang</a>, <a
                      href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a></i><br>

              <p>We propose a method to ablate (remove) copyrighted materials and memorized images from pretrained
                text-to-image generative models. Our algorithm changes the target concept distribution to an anchor
                concept, e.g., Van Gogh painting to paintings or Grumpy cat to Cat.
                <br>

              </p>
              <strong>ICCV 2023.</strong><br>
              [<a href="https://arxiv.org/abs/2303.13516">Paper</a>]
              [<a href="https://www.cs.cmu.edu/~concept-ablation/">Webpage</a>]
              [<a href="https://github.com/nupurkmr9/concept-ablation">Code</a>]
              </a> </p>
            </td>
          </tr>



          <tr>
            <td width="30%">
              <img id="img-opt" src="img/modelverse2022.jpg" alt="project_img" width="160" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><a href="https://modelverse.cs.cmu.edu/">
                  <papertitle>Content-Based Search for Deep Generative Models</papertitle>
                </a><br>
                <i><span style="font-size: 10pt;">
                    <a href="https://github.com/daohanlu">Daohan Lu*</a>, <a
                      href="https://peterwang512.github.io/">Sheng-Yu Wang*</a>,
                    Nupur Kumari*, <a href="https://www.ri.cmu.edu/ri-people/rohan-agarwal/">Rohan Agarwal*</a>, <a
                      href="https://www.mia-tang.com">Mia Tang</a>, <a
                      href="https://people.csail.mit.edu/davidbau/home/">David Bau</a>, <a
                      href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a></i><br>

              <p> We propose an algorithm for searching over generative models using image,text, and sketch.
                Our search platform is available at <a href="https://modelverse.cs.cmu.edu/">Modelverse</a>.
              </p>
              <strong>SIGGRAPH Asia 2023.</strong><br>
              [<a href="https://arxiv.org/abs/2210.03116">Paper</a>]
              [<a href="https://modelverse.cs.cmu.edu/about">Webpage</a>]
              [<a href="https://github.com/generative-intelligence-lab/modelverse">Code</a>]
              </a> </p>
            </td>
          </tr>


          <tr>
            <td width="30%">
              <img id="img-opt" src="img/vision_aided_gan.png" alt="project_img" width="160" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><a href="https://arxiv.org/abs/2112.09130">
                  <papertitle>Ensembling Off-the-shelf Models for GAN Training</papertitle>
                </a><br>
                <i><span style="font-size: 10pt;">
                    Nupur Kumari, <a href="https://richzhang.github.io/">Richard Zhang</a>, <a
                      href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a
                      href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a></i><br>

              <p>We show that pretrained computer vision models can significantly improve performance when used in an
                ensemble of discriminators.
                Our method improves FID by 1.5x to 2x on cat, church, and horse categories of LSUN.
                <br>

              </p>
              <strong>CVPR 2022 (Oral).</strong><br>
              [<a href="https://arxiv.org/abs/2112.09130">Paper</a>]
              [<a href="https://www.cs.cmu.edu/~vision-aided-gan/">Webpage</a>]
              [<a href="https://github.com/nupurkmr9/vision-aided-gan">Code</a>]

              </a> </p>
            </td>
          </tr>


          <tr>
            <td width="30%"><img id="img-opt" src="img/robust_attr.png" alt="project_img" width="160"
                style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><a href="https://arxiv.org/abs/1911.13073">
                  <papertitle>Attributional Robustness Training using Input-Gradient Spatial Alignment</papertitle>
                </a><br>
                <i><span style="font-size: 10pt;">
                    Nupur Kumari<sup>&#42;</sup>, <a href="https://msingh27.github.io/">Mayank
                      Singh</a><sup>&#42;</sup>, Puneet Mangla, <a href="https://a7b23.github.io/">Abhishek Sinha</a>,
                    <a href="https://www.iith.ac.in/~vineethnb/">Vineeth N Balasubramanian</a>, Balaji
                    Krishnamurthy</i><br>

              <p>We propose a robust attribution training methodology <i>ART</i> that maximizes the alignment between
                the input and its attribution map.
                <i>ART</i> achieves state-of-the-art performance in attributional robustness and weakly supervised
                object localization on CUB dataset.
                <br>
              </p>
              <strong>ECCV 2020. </strong><br>
              [<a href="https://arxiv.org/abs/1911.13073">Paper</a>]
              [<a href="https://nupurkmr9.github.io/Attributional-Robustness/">Webpage</a>]
              [<a href="https://github.com/nupurkmr9/Attributional-Robustness">Code</a>]

              </a> </p>
            </td>
          </tr>



          <tr>
            <td width="30%"><img id="img-opt" src="img/few_shot.png" alt="project_img" width="160"
                style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><a href="https://arxiv.org/pdf/1907.12087.pdf">
                  <papertitle>Charting the Right Manifold: Manifold Mixup for Few-shot Learning</papertitle>
                </a><br>
                <i><span style="font-size: 10pt;">
                    Puneet Mangla<sup>&#42;</sup>, Nupur Kumari<sup>&#42;</sup>, <a
                      href="https://a7b23.github.io/">Abhishek Sinha</a><sup>&#42;</sup>, <a
                      href="https://msingh27.github.io/">Mayank Singh</a><sup>&#42;</sup>, <a
                      href="https://www.iith.ac.in/~vineethnb/">Vineeth N Balasubramanian</a>, Balaji
                    Krishnamurthy</i><br>
              <p>Used self-supervision techniques - rotation and exemplar, followed by manifold mixup for few-shot
                classification tasks.
                The proposed approach beats the current state-of-the-art accuracy on mini-ImageNet, CUB and CIFAR-FS
                datasets by 3-8%.
                <br>
              </p>
              <strong>WACV 2020.</strong><br>
              [<a href="https://arxiv.org/pdf/1907.12087.pdf">Paper</a>]
              [<a href="https://github.com/nupurkmr9/S2M2_fewshot">Code</a>]
              </a> </p>
            </td>
          </tr>


          <table width="100%" align="left" border="0" cellspacing="0" cellpadding="20">
            <p align="left">
              <font size="2">
                <sup>&#42;</sup> denotes equal contribution
              </font>

          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <br>
                <p align="right">
                  <font size="2">
                    <a href="http://www.cs.berkeley.edu/~barron/">web template taken from this website</a>
                  </font>
                </p>
              </td>
            </tr>
          </table>
      </td>
    </tr>
  </table>

  <script type="text/javascript">
    var sc_project = 11673319;
    var sc_invisible = 1;
    var sc_security = "327094c7"; 
  </script>
  <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript>
    <div class="statcounter"><a title="Web Analytics
Made Easy - StatCounter" href="http://statcounter.com/" target="_blank"><img class="statcounter"
          src="//c.statcounter.com/11673319/0/327094c7/1/" alt="Web
Analytics Made Easy - StatCounter"></a></div>
  </noscript>
  <!-- End of Statcounter Code -->
</body>

</html>